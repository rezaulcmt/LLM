# -*- coding: utf-8 -*-
"""Large language models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T7KOQPt37RrA6-mnsszJ_zUouDBpVfTP

## **Part 1: Setup and Installation**
"""

# Install required libraries
!pip install -q transformers datasets torch sklearn pandas matplotlib seaborn
!pip install -q accelerate  # Required for current Hugging Face Trainer
!pip install --upgrade datasets huggingface_hub fsspec

"""# **Part 2: Data Loading and Preprocessing**"""

# Load and explore the IMDb dataset

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset



# Load with streaming mode which might avoid the filesystem issue
dataset = load_dataset('imdb', streaming=True)

# If you need the full dataset (not streamed), try:
dataset = load_dataset('imdb', download_mode="force_redownload")

# Convert to pandas DataFrames
train_df = pd.DataFrame(dataset['train'])
test_df = pd.DataFrame(dataset['test'])

# Display basic info
print("\nTraining samples:", len(train_df))
print("Test samples:", len(test_df))
print("\nSample training data:")
print(train_df.head(3))

# Visualize review length distribution
train_df['text_length'] = train_df['text'].apply(len)
plt.figure(figsize=(10, 5))
sns.histplot(train_df['text_length'], bins=50)
plt.title('Distribution of Review Length')
plt.xlabel('Number of Characters')
plt.ylabel('Frequency')
plt.show()

# Visualize class balance
plt.figure(figsize=(8, 4))
train_df['label'].value_counts().plot(kind='bar')
plt.title('Class Distribution in Training Set')
plt.xticks([0, 1], ['Negative', 'Positive'], rotation=0)
plt.ylabel('Count')
plt.show()

"""# **Part 3: Tokenization**"""

# Prepare text for BERT using Hugging Face tokenizer

from transformers import BertTokenizer

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Sample tokenization
sample_text = "This movie was absolutely wonderful!"
print("Original text:", sample_text)
print("Tokenized:", tokenizer.tokenize(sample_text))
print("Token IDs:", tokenizer.encode(sample_text))

# Tokenization function
def tokenize_function(batch):
    return tokenizer(
        batch['text'],
        padding='max_length',
        truncation=True,
        max_length=256,
        return_tensors='pt'
    )

# Apply to dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Rename columns for Trainer compatibility
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

# Show processed example
print("\nTokenized example structure:")
print({k: tokenized_datasets['train'][0][k] for k in ['input_ids', 'attention_mask', 'labels']})

"""# **Part 4: Model Initialization**"""

# Load pre-trained BERT with classification head

from transformers import BertForSequenceClassification

# Initialize model
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2  # Binary classification
)

# Print model architecture
print("Model architecture:")
print(model)

# Explain parameters
print("\nTotal parameters:", sum(p.numel() for p in model.parameters()))
print("Trainable parameters:", sum(p.numel() for p in model.parameters() if p.requires_grad))

"""# **Part 5: Training Setup**"""

# Configure training arguments and metrics

from transformers import TrainingArguments, Trainer
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

# Reduce dataset size for faster demo
small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(2000))
small_eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select(range(500))

# Metrics function
def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=1)

    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='binary')

    return {'accuracy': acc, 'f1': f1}

# Training configuration
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    eval_strategy="epoch",  # Older versions used this name
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics
)

"""# **Part 6: Model Training**"""

# Fine-tune BERT on our dataset

# Start training
print("Starting training...")
train_results = trainer.train()

# Save model
trainer.save_model('./sentiment_model')
tokenizer.save_pretrained('./sentiment_model')

# Training metrics
print("\nTraining completed. Metrics:")
print(pd.DataFrame([train_results.metrics]))

"""# **Part 7: Evaluation**"""

# Evaluate model performance

# Full evaluation on test set
print("Evaluating on test set...")
eval_results = trainer.evaluate(tokenized_datasets['test'].select(range(1000)))
print("\nTest set performance:")
print(f"Accuracy: {eval_results['eval_accuracy']:.4f}")
print(f"F1 Score: {eval_results['eval_f1']:.4f}")

# Confusion matrix
predictions = trainer.predict(tokenized_datasets['test'].select(range(1000)))
preds = np.argmax(predictions.predictions, axis=-1)
labels = predictions.label_ids

cm = confusion_matrix(labels, preds)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.savefig('confusion_matrix.png', bbox_inches='tight')
plt.show()

# Error analysis: Misclassified examples
test_subset = tokenized_datasets['test'].select(range(1000))
misclassified = []
for i in range(len(preds)):
    if preds[i] != labels[i]:
        text = dataset['test']['text'][i]
        misclassified.append({
            'text': text,
            'true_label': 'Positive' if labels[i] == 1 else 'Negative',
            'predicted_label': 'Positive' if preds[i] == 1 else 'Negative'
        })

print("\nSample misclassified reviews:")
for i, example in enumerate(misclassified[:3]):
    print(f"\nExample {i+1}:")
    print(f"Text: {example['text'][:200]}...")
    print(f"True: {example['true_label']}, Predicted: {example['predicted_label']}")

"""# **Part 8: Inference Demonstration**"""

import torch  # For CUDA device check
import pandas as pd  # For DataFrame
from IPython.display import display  # For pretty display in notebook

# Test model with custom inputs

from transformers import pipeline

# Create sentiment analysis pipeline
classifier = pipeline(
    'text-classification',
    model='./sentiment_model',
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1
)

# Sample reviews
sample_reviews = [
    "This movie was absolutely fantastic! The acting was superb and the plot was incredibly engaging from start to finish.",
    "A complete waste of time. I've never seen such poor acting and disjointed storytelling in my life.",
    "While the cinematography was beautiful, the characters felt underdeveloped and the pacing was too slow.",
    "This is the best film I've seen this year. The director's vision was executed perfectly by the entire cast.",
    "I expected so much more from this acclaimed director. The plot holes were big enough to drive a truck through."
]

# Run predictions
results = classifier(sample_reviews)

# Display results
print("\nSample Predictions:")
for i, (review, result) in enumerate(zip(sample_reviews, results)):
    sentiment = 'Positive' if result['label'] == 'LABEL_1' else 'Negative'
    print(f"\nReview {i+1}:")
    print(f"Text: {review[:100]}...")
    print(f"Sentiment: {sentiment} (Confidence: {result['score']:.4f})")

# Create comparison table
comparison_df = pd.DataFrame({
    'Review': [r[:80] + "..." for r in sample_reviews],
    'Prediction': ['Positive' if r['label'] == 'LABEL_1' else 'Negative' for r in results],
    'Confidence': [r['score'] for r in results]
})
print("\nPrediction Summary Table:")
display(comparison_df)

#Prediction

from matplotlib import pyplot as plt
import seaborn as sns
comparison_df.groupby('Prediction').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# Review

from matplotlib import pyplot as plt
import seaborn as sns
comparison_df.groupby('Review').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# Confidence

from matplotlib import pyplot as plt
comparison_df['Confidence'].plot(kind='hist', bins=20, title='Confidence')
plt.gca().spines[['top', 'right',]].set_visible(False)

